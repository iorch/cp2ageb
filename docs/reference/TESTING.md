# Suite de Tests - cp2ageb

Documentaci√≥n completa de la arquitectura de tests del proyecto cp2ageb.

## Resumen

El proyecto incluye una suite completa de tests que cubre:
- ‚úÖ Tests unitarios (no requieren infraestructura)
- ‚úÖ Tests de base de datos (estructura y configuraci√≥n)
- ‚úÖ Tests de integraci√≥n (flujo completo CP ‚Üí AGEB)
- ‚úÖ Tests de performance
- ‚úÖ Tests de consistencia de datos

**Total de tests**: ~45 casos de prueba
**Coverage objetivo**: >80% en c√≥digo cr√≠tico

## Arquitectura de Tests

### Organizaci√≥n de Archivos

```
tests/
‚îú‚îÄ‚îÄ __init__.py                 # Paquete de tests
‚îú‚îÄ‚îÄ conftest.py                 # Configuraci√≥n pytest y fixtures
‚îú‚îÄ‚îÄ test_database.py            # Tests de base de datos (15 tests)
‚îú‚îÄ‚îÄ test_scripts.py             # Tests de scripts Python (15 tests)
‚îú‚îÄ‚îÄ test_integration.py         # Tests de integraci√≥n (15 tests)
‚îî‚îÄ‚îÄ README.md                   # Documentaci√≥n de tests

Archivos de configuraci√≥n:
‚îú‚îÄ‚îÄ pytest.ini                  # Configuraci√≥n pytest
‚îú‚îÄ‚îÄ requirements-test.txt       # Dependencias de tests
‚îî‚îÄ‚îÄ run_tests.sh                # Script para ejecutar tests
```

## Tests Implementados

### 1. test_database.py (15 tests)

**Prop√≥sito**: Verificar configuraci√≥n y estructura de la base de datos

#### TestDatabaseConnection (4 tests)
- `test_database_exists`: Conexi√≥n a PostgreSQL
- `test_postgis_extension`: PostGIS instalado
- `test_schemas_exist`: Schemas sepomex e inegi
- `test_metadata_table_exists`: Tabla load_metadata

#### TestDataLoading (4 tests)
- `test_sepomex_tables_loaded`: Tablas SEPOMEX presentes
- `test_inegi_tables_loaded`: Tablas INEGI presentes
- `test_ageb_tables_pattern`: Patr√≥n de nombres correcto
- `test_sepomex_table_structure`: Columnas esperadas (d_cp, geom)
- `test_inegi_table_structure`: Columnas esperadas (cvegeo, geom)

#### TestSpatialQueries (3 tests)
- `test_geometries_are_valid`: ST_IsValid() en todas las geometr√≠as
- `test_spatial_indexes_exist`: √çndices GIST presentes
- `test_srid_consistency`: SRIDs correctos (900917, 900919, 6372)

#### TestBuscarAgebsPorCPFunction (4 tests)
- `test_function_exists`: Funci√≥n buscar_agebs_por_cp existe
- `test_function_returns_data`: Retorna datos para CPs conocidos
- `test_function_with_invalid_cp`: Maneja CPs inv√°lidos
- `test_function_return_structure`: Estructura de retorno correcta

**Ejecuci√≥n**:
```bash
./run_tests.sh --database
```

### 2. test_scripts.py (15 tests)

**Prop√≥sito**: Verificar scripts Python y funciones helper

#### TestDownloadShapefilesScript (5 tests)
- `test_estados_dict_complete`: 32 estados definidos
- `test_estados_dict_structure`: Estructura del diccionario
- `test_estados_unique_abbreviations`: Abreviaturas √∫nicas
- `test_base_url_format`: URL v√°lida
- `test_download_file_success`: Mock de descarga exitosa
- `test_download_file_already_exists`: No redescarga si existe
- `test_download_file_404_error`: Manejo de errores HTTP

#### TestDownloadAgebShapefilesScript (4 tests)
- `test_estados_list_complete`: 32 estados definidos
- `test_estados_list_structure`: Formato (c√≥digo, nombre, completo)
- `test_estados_unique_codes`: C√≥digos √∫nicos
- `test_estados_sequential_codes`: C√≥digos 01-32
- `test_base_url_format`: URL v√°lida

#### TestLoadShapefilesHelpers (7 tests)
- `test_normalize_estado_by_code`: Normalizaci√≥n por c√≥digo (1, 01, 14)
- `test_normalize_estado_by_abbreviation`: Por abreviatura (Jal, jal)
- `test_normalize_estado_by_name`: Por nombre (Jalisco, jalisco)
- `test_normalize_estado_invalid`: Manejo de inv√°lidos
- `test_parse_estados_filter_single`: Parsing de un estado
- `test_parse_estados_filter_multiple`: Parsing m√∫ltiples
- `test_parse_estados_filter_mixed`: Formatos mixtos
- `test_parse_estados_filter_all`: Valor 'all'

#### TestDataIntegrity (2 tests)
- `test_estados_consistency_between_scripts`: 32 en ambos scripts
- `test_file_paths_logic`: Construcci√≥n de paths

**Ejecuci√≥n**:
```bash
./run_tests.sh --unit
```

### 3. test_integration.py (15 tests)

**Prop√≥sito**: Tests end-to-end del flujo completo

#### TestEndToEndCPtoAGEB (3 tests)
- `test_known_cp_returns_agebs`: CPs conocidos (44100, 11560, 50000, 64000)
  - Verifica estructura de resultados
  - Verifica porcentajes suman ~100%
  - Verifica orden descendente
- `test_spatial_intersection_quality`: Calidad de intersecciones
  - AGEB principal >10%
  - Sin intersecciones triviales
- `test_ageb_codes_format`: Formato de claves AGEB
  - Longitud correcta (‚â•12 caracteres)
  - Primeros 2 d√≠gitos num√©ricos

#### TestDataConsistency (2 tests)
- `test_states_match_between_sources`: Estados coinciden SEPOMEX/INEGI
- `test_ageb_pairs_exist`: Cada estado tiene urbanas Y rurales

#### TestPerformance (2 tests)
- `test_function_query_speed`: Query individual <2s
- `test_multiple_queries_speed`: Promedio m√∫ltiples queries <1s

**Ejecuci√≥n**:
```bash
./run_tests.sh --integration
```

## Fixtures Compartidos

**Definidos en**: `conftest.py`

### Fixtures de Infraestructura

#### `docker_available` (session scope)
Verifica que Docker est√° disponible y contenedor corriendo.

```python
@pytest.fixture(scope="session")
def docker_available():
    # Verifica docker-compose ps
    return 'cp2ageb-postgis' in output and 'Up' in output
```

#### `database_available` (session scope)
Verifica que PostgreSQL acepta conexiones.

```python
@pytest.fixture(scope="session")
def database_available(docker_available):
    # Intenta conectar a PostgreSQL
    # Skip tests si no est√° disponible
```

### Fixtures de Conexi√≥n

#### `db_conn` (function scope)
Proporciona conexi√≥n a PostgreSQL para cada test.

```python
@pytest.fixture
def db_conn():
    conn = psycopg2.connect(...)
    yield conn
    conn.close()  # Limpieza autom√°tica
```

### Fixtures de Configuraci√≥n

#### `test_config` (session scope)
Configuraci√≥n centralizada para tests.

```python
@pytest.fixture(scope="session")
def test_config():
    return {
        'postgres_host': 'localhost',
        'postgres_port': '5432',
        'test_cps': ['44100', '11560', '50000', '64000'],
        'test_estados': ['14', '15', '09', '19'],
    }
```

#### `sample_cp` (function scope)
C√≥digo postal de ejemplo.

```python
@pytest.fixture
def sample_cp():
    return '44100'  # Guadalajara, Jalisco
```

## Markers de Pytest

Los tests est√°n categorizados con markers:

### Markers Principales

| Marker | Descripci√≥n | Cantidad |
|--------|-------------|----------|
| `unit` | Tests unitarios, no requieren DB | ~15 |
| `integration` | Tests de integraci√≥n end-to-end | ~15 |
| `database` | Tests de base de datos | ~15 |
| `slow` | Tests que pueden tardar >2s | ~5 |

### Uso de Markers

```bash
# Solo tests unitarios
pytest -m unit

# Excluir tests lentos (default)
pytest -m "not slow"

# Solo tests de base de datos
pytest -m database

# Integraci√≥n sin lentos
pytest -m "integration and not slow"
```

## Configuraci√≥n de Pytest

**Archivo**: `pytest.ini`

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

addopts =
    -v
    --strict-markers
    --tb=short
    --disable-warnings
    -ra
    --color=yes

markers =
    unit: Unit tests
    integration: Integration tests
    slow: Slow tests
    database: Database tests
```

## Script de Ejecuci√≥n

**Archivo**: `run_tests.sh`

### Opciones Disponibles

```bash
./run_tests.sh [opciones]

--all               Todos los tests (default)
--unit              Solo unitarios
--integration       Solo integraci√≥n
--database          Solo base de datos
--slow              Incluir lentos
--no-slow           Excluir lentos (default)
--coverage          Generar reporte coverage
--parallel          Ejecutar en paralelo
--verbose, -v       Output detallado
--help, -h          Ayuda
```

### Ejemplos de Uso

```bash
# B√°sico - todos excepto lentos
./run_tests.sh

# Solo unitarios (r√°pido, no requiere DB)
./run_tests.sh --unit

# Integraci√≥n con coverage
./run_tests.sh --integration --coverage

# Todo en paralelo
./run_tests.sh --all --parallel

# Todo incluyendo lentos
./run_tests.sh --all --slow
```

## Dependencias de Tests

**Archivo**: `requirements-test.txt`

```txt
pytest>=7.4.0
pytest-cov>=4.1.0
pytest-xdist>=3.3.1
psycopg2-binary>=2.9.6
pytest-mock>=3.11.1
requests-mock>=1.11.0
coverage>=7.2.7
```

**Instalaci√≥n**:
```bash
pip install -r requirements-test.txt
```

## Estrategia de Testing

### Pir√°mide de Tests

```
        /\
       /  \          2 tests - Performance
      /____\
     /      \        15 tests - Integration (E2E)
    /________\
   /          \      15 tests - Database
  /____________\
 /              \    15 tests - Unit
/________________\
```

### Niveles de Testing

#### Nivel 1: Unit Tests (R√°pidos, ~0.1s cada uno)
- No requieren infraestructura
- Ejecutan l√≥gica pura
- Ideal para TDD
- Se ejecutan en CI en cada commit

#### Nivel 2: Database Tests (Medios, ~0.5s cada uno)
- Requieren PostgreSQL
- Verifican estructura
- Detectan problemas de schema
- Se ejecutan en CI antes de merge

#### Nivel 3: Integration Tests (Lentos, ~1-2s cada uno)
- Flujo completo CP ‚Üí AGEB
- Verifican funcionalidad real
- Detectan problemas de integraci√≥n
- Se ejecutan en CI antes de release

#### Nivel 4: Performance Tests (Muy lentos, ~5-10s cada uno)
- Miden tiempos de respuesta
- Detectan regresiones de performance
- Se ejecutan manualmente o nightly

## Coverage

### Objetivo de Coverage

| Componente | Objetivo | Actual |
|-----------|----------|--------|
| Scripts Python | >90% | TBD |
| Funci√≥n SQL | >80% | TBD |
| Queries espaciales | >70% | TBD |
| **Global** | **>80%** | **TBD** |

### Generar Reporte

```bash
# Ejecutar con coverage
./run_tests.sh --coverage

# Ver reporte HTML
open htmlcov/index.html

# Ver reporte en terminal
coverage report
```

## CI/CD Integration

### GitHub Actions Example

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Start Docker services
      run: |
        docker-compose up -d
        sleep 30  # Wait for initialization

    - name: Run unit tests
      run: ./run_tests.sh --unit

    - name: Run integration tests
      run: ./run_tests.sh --integration --coverage

    - name: Upload coverage
      uses: codecov/codecov-action@v3
```

## Troubleshooting

### Tests Fallan: "Database not available"

**Causa**: Contenedor Docker no est√° corriendo

**Soluci√≥n**:
```bash
docker-compose up -d
docker-compose ps  # Verificar estado
```

### Tests se Omiten: "No hay datos para probar"

**Causa**: Estado espec√≠fico no est√° cargado

**Esperado**: Los tests se omiten (skip) cuando los datos no est√°n
disponibles. Esto es v√°lido.

**Para cargar m√°s datos**:
```bash
# Editar docker-compose.yml: LOAD_ESTADOS
docker-compose down -v
docker-compose up -d
```

### Tests Muy Lentos

**Soluci√≥n 1**: Excluir tests lentos
```bash
./run_tests.sh --no-slow
```

**Soluci√≥n 2**: Ejecutar en paralelo
```bash
./run_tests.sh --parallel
```

**Soluci√≥n 3**: Solo unitarios
```bash
./run_tests.sh --unit  # ~2 segundos total
```

### Error: "pytest: command not found"

**Soluci√≥n**:
```bash
pip install -r requirements-test.txt
```

## Mejores Pr√°cticas

### Al Escribir Tests

1. **Usar fixtures apropiados**
   ```python
   def test_con_db(db_conn):
       with db_conn.cursor() as cur:
           cur.execute("SELECT 1;")
   ```

2. **Agregar markers**
   ```python
   @pytest.mark.integration
   def test_flujo_completo():
       pass
   ```

3. **Tests descriptivos**
   ```python
   def test_buscar_agebs_retorna_porcentajes_correctos():
       # Nombre describe qu√© se prueba
       pass
   ```

4. **Limpiar despu√©s**
   ```python
   @pytest.fixture
   def recurso():
       r = crear_recurso()
       yield r
       r.close()  # Limpieza autom√°tica
   ```

### Al Ejecutar Tests

1. **Tests r√°pidos primero**
   ```bash
   ./run_tests.sh --unit  # R√°pido
   ```

2. **Coverage en desarrollo**
   ```bash
   ./run_tests.sh --coverage
   ```

3. **Paralelo en CI**
   ```bash
   ./run_tests.sh --parallel
   ```

## Roadmap de Tests

### Implementado ‚úÖ
- [x] Tests de base de datos
- [x] Tests de scripts Python
- [x] Tests de integraci√≥n E2E
- [x] Tests de performance b√°sicos
- [x] Script de ejecuci√≥n con opciones
- [x] Configuraci√≥n pytest completa
- [x] Fixtures compartidos

### Futuro üìã
- [ ] Tests de API REST (si se implementa)
- [ ] Tests de visualizaci√≥n (si se implementa)
- [ ] Tests de carga (stress testing)
- [ ] Tests de seguridad
- [ ] Property-based testing con Hypothesis
- [ ] Mutation testing

## M√©tricas de Calidad

### Objetivos

- ‚úÖ >45 tests implementados
- ‚úÖ 3 tipos de tests (unit, database, integration)
- ‚úÖ Coverage >80%
- ‚úÖ Tests ejecutan <30s (sin slow)
- ‚úÖ 100% tests pasan en CI
- ‚úÖ 0 flaky tests

## Recursos Adicionales

- [Documentaci√≥n pytest](https://docs.pytest.org/)
- [pytest-cov](https://pytest-cov.readthedocs.io/)
- [Testing Best Practices](https://docs.python-guide.org/writing/tests/)
- [PostGIS Testing](https://postgis.net/docs/manual-dev/RT_FAQ.html)

## Contribuir

Para agregar nuevos tests:

1. Crear archivo `test_*.py` en `tests/`
2. Usar fixtures de `conftest.py`
3. Agregar markers apropiados
4. Documentar en `tests/README.md`
5. Ejecutar suite completa
6. Verificar coverage

---

**√öltima actualizaci√≥n**: 2025-11-07
**Versi√≥n suite de tests**: 1.0.0
